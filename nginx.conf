events { 
    worker_connections 4096; # Increased from 1024 to handle high concurrency
    use epoll; # Efficient event processing for Linux
    multi_accept on;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    # --- 1. GZIP COMPRESSION ---
    # Drastically reduces JSON payload size (often 70-90% smaller)
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6; # Balance between CPU usage and compression
    gzip_min_length 256;
    gzip_types 
        text/plain 
        text/css 
        text/xml 
        application/json 
        application/javascript 
        application/xml+rss 
        application/atom+xml 
        image/svg+xml;

    # --- 2. CACHE ZONE SETUP ---
    # Define a cache zone named 'api_cache' in RAM/Disk
    # 10m keys_zone = ~80,000 keys
    # max_size=100m = Cache up to 100MB of responses
    # inactive=60m = Delete items not accessed in 60 minutes
    proxy_cache_path /var/cache/nginx/api_cache levels=1:2 keys_zone=api_cache:10m max_size=100m inactive=60m use_temp_path=off;

    # Upstream definition for your Node.js app
    upstream api_nodes {
        server api:3000;
        
        # sticky sessions (ip_hash) are strictly required for Socket.io
        ip_hash;
        
        keepalive 64;
    }

    server {
        listen 80;
        server_name localhost;

        # Standard Proxy Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # --- 3. MICRO-CACHING LOGIC ---
        location /api/ {
            proxy_pass http://api_nodes;
            
            # Enable the cache defined above
            proxy_cache api_cache;
            
            # CACHE LOCK: Critical for stampede protection
            # If 1000 requests come for the same URL at once, only let 1 through 
            # to the backend; make others wait for that one response.
            proxy_cache_lock on;
            proxy_cache_lock_timeout 5s;

            # Cache valid 200 responses for 1 second (Micro-caching)
            # This protects the backend from massive spikes while keeping data "live"
            proxy_cache_valid 200 1s;
            
            # Use stale cache if backend is busy or erroring (High Availability)
            # If Node.js crashes, Nginx serves the last known good response
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            proxy_cache_background_update on;
            
            # Bypass cache for specific headers (useful for debugging)
            proxy_cache_bypass $http_x_no_cache;

            # Add header to debug cache status (HIT, MISS, BYPASS)
            add_header X-Cache-Status $upstream_cache_status;
        }

        # WebSocket / Socket.io (DO NOT CACHE)
        location /socket.io/ {
            proxy_pass http://api_nodes;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Disable buffering/caching for WebSockets
            proxy_buffering off;
            proxy_cache off;
            
            proxy_read_timeout 3600s;
            proxy_send_timeout 3600s;
        }

        # Fallback for other routes (admin, health, etc)
        location / {
            proxy_pass http://api_nodes;
        }
    }
}
```
